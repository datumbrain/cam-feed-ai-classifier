# cam-feed-ai-classifier

A real-time AI-powered camera feed classifier that uses YOLOv8 to detect and classify whether people are actively working at their desks. Supports both MacBook built-in cameras and iPhone cameras via Continuity Camera.

![Python](https://img.shields.io/badge/python-v3.10+-blue.svg)
![OpenCV](https://img.shields.io/badge/opencv-4.8+-green.svg)
![YOLOv8](https://img.shields.io/badge/YOLOv8-ultralytics-orange.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

## üöÄ Features

- **Real-time AI Classification**: Live desk work activity detection using YOLOv8
- **Multi-Camera Support**: Works with MacBook cameras and iPhone cameras
- **Person & Equipment Detection**: Identifies people, laptops, keyboards, mice, and phones
- **Pose Analysis**: Advanced body posture analysis for accurate work state detection
- **Live Statistics**: Real-time counts of working vs. idle people
- **High Performance**: Optimized frame processing with adjustable quality modes

## üìä Classifications

- üü¢ **Working**: Person at desk with active working posture
- üü° **At Desk**: Person at desk but not actively engaged
- üü† **At Desk (Idle)**: Person at desk with minimal activity
- üî¥ **Away from Desk**: Person not positioned at workspace

## üõ†Ô∏è Installation

### Prerequisites

- Python 3.8+
- macOS (tested on macOS 13+)
- Camera access permissions

### Setup

1. **Clone the repository**

   ```bash
   git clone https://github.com/yourusername/cam-feed-ai-classifier.git
   cd cam-feed-ai-classifier
   ```

2. **Install dependencies**

   ```bash
   # Using pipenv (recommended)
   pipenv install

   # Or using pip
   pip install -r requirements.txt
   ```

3. **Grant camera permissions**
   - **System Preferences** ‚Üí **Security & Privacy** ‚Üí **Camera**
   - Enable access for Terminal/IDE

## üéØ Quick Start

### MacBook Camera

```bash
pipenv shell
python main.py
```

### iPhone Camera Setup

#### **Method 1: Continuity Camera (macOS 13+)**

1. iPhone: Settings ‚Üí General ‚Üí AirPlay & Handoff ‚Üí Continuity Camera ‚úì
2. Mac: System Settings ‚Üí General ‚Üí AirPlay & Handoff ‚Üí iPhone Widgets ‚úì
3. Keep iPhone unlocked and nearby

#### **Method 2: Third-party Apps**

- Install [Camo](https://reincubate.com/camo/) or [EpocCam](https://www.elgato.com/epoccam)
- Connect via USB or WiFi

Then run:

```bash
pipenv run python main.py
```

## ‚å®Ô∏è Controls

| Key | Action                      |
| --- | --------------------------- |
| `q` | Quit application            |
| `s` | Save current frame          |
| `t` | Test camera connection      |
| `f` | Toggle full processing mode |
| `c` | Show available cameras      |
| `r` | Reset/restart detection     |

## üìÅ Project Structure

```raw
cam-feed-ai-classifier/
‚îú‚îÄ‚îÄ LICENSE                 # Project license
‚îú‚îÄ‚îÄ Pipfile                # Pipenv dependencies
‚îú‚îÄ‚îÄ Pipfile.lock          # Locked dependency versions
‚îú‚îÄ‚îÄ main.py               # Main application
‚îú‚îÄ‚îÄ pyproject.toml        # Project configuration
‚îú‚îÄ‚îÄ .gitignore           # Git ignore rules
‚îú‚îÄ‚îÄ .pre-commit-config.yaml # Pre-commit hooks
‚îî‚îÄ‚îÄ README.md            # This file
```

## üîß Configuration

### Camera Detection

The system automatically detects available cameras:

- **Index 0**: MacBook built-in camera (1280x720)
- **Index 1**: iPhone/external camera (1920x1080+)
- **Index 2+**: Additional cameras

### Adjusting Sensitivity

Edit confidence thresholds in `main.py`:

```python
# Object detection confidence
obj_results = self.yolo_model.predict(frame, conf=0.3)

# Working pose threshold
is_working = working_score > 0.5  # Adjust 0.1-0.9
```

### Performance Tuning

```python
# Frame processing (every Nth frame)
skip_frames = 2  # Process every 3rd frame

# Camera resolution
self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)   # Higher = better quality
self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)  # Higher = slower processing
```

## üß† How It Works

### AI Pipeline

1. **Object Detection** (YOLOv8): Detects persons and work equipment
2. **Spatial Analysis**: Determines desk positioning relationships
3. **Pose Estimation** (YOLOv8-pose): Analyzes body keypoints
4. **Classification Logic**: Combines spatial + pose data for final status
5. **Real-time Display**: Live visualization with statistics

### Detection Logic

- **Proximity Check**: Person within range of work equipment
- **Posture Analysis**: Sitting position, shoulder alignment, head orientation
- **Hand Position**: Typing posture detection via wrist/elbow keypoints
- **Engagement Score**: Combined confidence metric (0.0-1.0)

## üéõÔ∏è Output Display

The interface shows:

- Live camera feed with bounding boxes
- Person classifications with confidence scores
- Equipment detection highlights
- Real-time statistics (People | At Desk | Working)
- Processing mode and timestamp
- Camera resolution info

## üêõ Troubleshooting

### Camera Issues

```bash
# Test camera availability
python -c "import cv2; print([cv2.VideoCapture(i).isOpened() for i in range(5)])"
```

**Common Solutions:**

- Close other camera apps (Zoom, Teams, etc.)
- Restart Terminal/IDE after granting permissions
- Try different camera indices (0, 1, 2)
- For iPhone: ensure same Apple ID, WiFi/Bluetooth enabled

### Performance Issues

- Use MacBook camera for better performance
- Reduce resolution in camera settings
- Increase frame skipping (`skip_frames = 3`)
- Toggle full processing mode with `f` key

### Detection Quality

- Ensure good lighting conditions
- Position camera to capture full desk area
- Use iPhone camera for higher resolution
- Adjust confidence thresholds

## üî¨ Technical Details

### Dependencies

- **ultralytics**: YOLOv8 models for detection and pose estimation
- **opencv-python**: Computer vision and camera handling
- **numpy**: Numerical computations for pose analysis

### Models

- **yolov8n.pt**: Lightweight object detection (~6MB)
- **yolov8n-pose.pt**: Human pose estimation (~6MB)

Models are automatically downloaded on first run.

### COCO Classes Used

- Class 0: Person
- Class 56: Chair
- Class 63: Laptop
- Class 64: Mouse
- Class 66: Keyboard
- Class 67: Cell phone

## üìä Performance

**Typical Performance:**

- MacBook Camera: ~15-20 FPS (720p)
- iPhone Camera: ~10-15 FPS (1080p)
- Detection Accuracy: ~85-90% in good lighting

**System Requirements:**

- RAM: 4GB+ recommended
- CPU: Modern Intel/Apple Silicon
- Storage: 200MB for models and dependencies

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

Please run pre-commit hooks before submitting:

```bash
pre-commit run --all-files
```

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- [Ultralytics](https://ultralytics.com/) for YOLOv8 models
- [OpenCV](https://opencv.org/) for computer vision capabilities
- COCO dataset for training data

## ‚ö†Ô∏è Privacy Notice

This tool processes camera feeds locally. Please respect privacy laws and obtain consent when monitoring individuals in workplace environments.

---

Made with ‚ù§Ô∏è at Datum Brain for productivity insights.
